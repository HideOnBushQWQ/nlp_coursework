# 中英双语命名实体识别系统 - 详细设计文档

## 1. 数据预处理模块详细设计

### 1.1 数据加载器 (dataset_loader.py)

#### 1.1.1 类设计

```python
class CoNLLDatasetLoader:
    """CoNLL格式数据集加载器（适用于CoNLL-2003和MSRA）"""
    
    def __init__(self, file_path: str, encoding: str = 'utf-8'):
        """
        Args:
            file_path: 数据文件路径
            encoding: 文件编码
        """
        self.file_path = file_path
        self.encoding = encoding
    
    def load(self) -> Tuple[List[List[str]], List[List[str]]]:
        """
        加载数据集
        Returns:
            sentences: 句子列表，每个句子是词/字的列表
            labels: 标签列表，每个标签序列对应一个句子
        """
        pass
    
    def _parse_line(self, line: str) -> Tuple[str, str]:
        """解析单行数据"""
        pass
```

**关键逻辑**：
1. 读取文件，按空行分隔句子
2. 每行格式：`token\tlabel` 或 `token label`
3. 处理异常情况：空行、注释行、格式错误
4. 返回平行的token序列和label序列

#### 1.1.2 数据格式示例

**CoNLL-2003格式**：
```
EU B-ORG
rejects O
German B-MISC
call O

Peter B-PER
Blackburn I-PER
```

**MSRA格式**：
```
中 B-LOC
国 I-LOC
是 O
一 O
个 O
```

### 1.2 NER数据集类 (dataset_loader.py)

#### 1.2.1 PyTorch Dataset实现

```python
class NERDataset(Dataset):
    """NER任务的PyTorch数据集"""
    
    def __init__(
        self,
        sentences: List[List[str]],
        labels: List[List[str]],
        tokenizer,
        label_encoder,
        max_length: int = 128
    ):
        """
        Args:
            sentences: 句子列表
            labels: 标签列表
            tokenizer: 分词器（HuggingFace tokenizer）
            label_encoder: 标签编码器
            max_length: 最大序列长度
        """
        self.sentences = sentences
        self.labels = labels
        self.tokenizer = tokenizer
        self.label_encoder = label_encoder
        self.max_length = max_length
    
    def __len__(self) -> int:
        return len(self.sentences)
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        """
        Returns:
            {
                'input_ids': tensor,
                'attention_mask': tensor,
                'labels': tensor
            }
        """
        pass
    
    def _align_labels(
        self,
        tokens: List[str],
        labels: List[str],
        word_ids: List[int]
    ) -> List[str]:
        """
        对齐标签到子词
        Args:
            tokens: 原始token列表
            labels: 原始标签列表
            word_ids: tokenizer返回的word_ids
        Returns:
            对齐后的标签列表
        """
        pass
```

**关键逻辑**：

1. **分词处理**：
```python
encoding = tokenizer(
    tokens,
    is_split_into_words=True,
    max_length=max_length,
    padding='max_length',
    truncation=True,
    return_offsets_mapping=True
)
```

2. **标签对齐**：
```python
aligned_labels = []
previous_word_id = None
for word_id in word_ids:
    if word_id is None:  # 特殊token
        aligned_labels.append(-100)
    elif word_id != previous_word_id:  # 新词的第一个子词
        aligned_labels.append(label_encoder.encode(labels[word_id]))
    else:  # 新词的后续子词
        aligned_labels.append(-100)
    previous_word_id = word_id
```

### 1.3 标签编码器 (label_encoder.py)

#### 1.3.1 类设计

```python
class LabelEncoder:
    """标签编码器：标签 <-> ID 转换"""
    
    def __init__(self, labels: List[str] = None):
        """
        Args:
            labels: 标签列表，如 ['O', 'B-PER', 'I-PER', ...]
        """
        if labels is None:
            # 默认标签：O + 4类实体的B/I
            labels = ['O']
            for entity_type in ['PER', 'LOC', 'ORG', 'MISC']:
                labels.extend([f'B-{entity_type}', f'I-{entity_type}'])
        
        self.label2id = {label: idx for idx, label in enumerate(labels)}
        self.id2label = {idx: label for label, idx in self.label2id.items()}
    
    def encode(self, label: str) -> int:
        """标签 -> ID"""
        return self.label2id.get(label, 0)  # 默认返回O的ID
    
    def encode_batch(self, labels: List[str]) -> List[int]:
        """批量编码"""
        return [self.encode(label) for label in labels]
    
    def decode(self, label_id: int) -> str:
        """ID -> 标签"""
        return self.id2label.get(label_id, 'O')
    
    def decode_batch(self, label_ids: List[int]) -> List[str]:
        """批量解码"""
        return [self.decode(lid) for lid in label_ids]
    
    @property
    def num_labels(self) -> int:
        """标签数量"""
        return len(self.label2id)
```

### 1.4 数据增强 (可选)

```python
class NERDataAugmenter:
    """NER数据增强"""
    
    def entity_replacement(
        self,
        sentence: List[str],
        labels: List[str],
        entity_dict: Dict[str, List[str]]
    ) -> Tuple[List[str], List[str]]:
        """实体替换增强"""
        pass
    
    def random_deletion(
        self,
        sentence: List[str],
        labels: List[str],
        p: float = 0.1
    ) -> Tuple[List[str], List[str]]:
        """随机删除（保持实体完整）"""
        pass
```

## 2. 模型模块详细设计

### 2.1 模型基类 (base_model.py)

```python
from abc import ABC, abstractmethod
import torch
import torch.nn as nn

class BaseNERModel(nn.Module, ABC):
    """NER模型抽象基类"""
    
    def __init__(self, config: Dict):
        """
        Args:
            config: 模型配置字典
        """
        super().__init__()
        self.config = config
        self.num_labels = config.get('num_labels', 9)
    
    @abstractmethod
    def forward(
        self,
        input_ids: torch.Tensor,
        attention_mask: torch.Tensor,
        labels: Optional[torch.Tensor] = None
    ) -> Dict[str, torch.Tensor]:
        """
        前向传播
        Args:
            input_ids: [batch_size, seq_len]
            attention_mask: [batch_size, seq_len]
            labels: [batch_size, seq_len]
        Returns:
            {
                'loss': tensor (if labels provided),
                'logits': tensor [batch_size, seq_len, num_labels],
                'predictions': tensor [batch_size, seq_len]
            }
        """
        pass
    
    @torch.no_grad()
    def predict(
        self,
        input_ids: torch.Tensor,
        attention_mask: torch.Tensor
    ) -> torch.Tensor:
        """
        推理预测
        Returns:
            predictions: [batch_size, seq_len]
        """
        self.eval()
        outputs = self.forward(input_ids, attention_mask)
        return outputs['predictions']
    
    def save_pretrained(self, save_dir: str):
        """保存模型"""
        os.makedirs(save_dir, exist_ok=True)
        torch.save(self.state_dict(), os.path.join(save_dir, 'model.pt'))
        with open(os.path.join(save_dir, 'config.json'), 'w') as f:
            json.dump(self.config, f, indent=2)
    
    @classmethod
    def from_pretrained(cls, save_dir: str):
        """加载模型"""
        with open(os.path.join(save_dir, 'config.json'), 'r') as f:
            config = json.load(f)
        model = cls(config)
        model.load_state_dict(torch.load(os.path.join(save_dir, 'model.pt')))
        return model
```

### 2.2 BERT-NER模型 (bert_ner.py)

#### 2.2.1 模型架构

```python
from transformers import BertModel, BertPreTrainedModel
from torchcrf import CRF

class BertNER(BaseNERModel):
    """BERT + CRF NER模型"""
    
    def __init__(self, config: Dict):
        """
        Args:
            config: {
                'pretrained_model': 'bert-base-chinese',
                'num_labels': 9,
                'dropout': 0.1,
                'use_crf': True
            }
        """
        super().__init__(config)
        
        # BERT编码器
        self.bert = BertModel.from_pretrained(
            config['pretrained_model']
        )
        
        # Dropout
        self.dropout = nn.Dropout(config.get('dropout', 0.1))
        
        # 分类层
        self.classifier = nn.Linear(
            self.bert.config.hidden_size,
            self.num_labels
        )
        
        # CRF层（可选）
        self.use_crf = config.get('use_crf', True)
        if self.use_crf:
            self.crf = CRF(self.num_labels, batch_first=True)
    
    def forward(
        self,
        input_ids: torch.Tensor,
        attention_mask: torch.Tensor,
        labels: Optional[torch.Tensor] = None
    ) -> Dict[str, torch.Tensor]:
        """前向传播"""
        
        # BERT编码
        outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask
        )
        sequence_output = outputs.last_hidden_state  # [B, L, H]
        
        # Dropout + 分类
        sequence_output = self.dropout(sequence_output)
        logits = self.classifier(sequence_output)  # [B, L, num_labels]
        
        # 损失计算和预测
        loss = None
        predictions = None
        
        if self.use_crf:
            # 使用CRF
            if labels is not None:
                # 创建mask（排除padding和特殊标签-100）
                mask = (labels != -100) & (attention_mask == 1)
                # 将-100替换为0（CRF不接受负数标签）
                labels_for_crf = labels.clone()
                labels_for_crf[labels == -100] = 0
                # 计算负对数似然
                loss = -self.crf(logits, labels_for_crf, mask=mask, reduction='mean')
            
            # CRF解码
            mask = (attention_mask == 1)
            predictions = torch.tensor(self.crf.decode(logits, mask=mask))
        else:
            # 不使用CRF
            if labels is not None:
                loss_fct = nn.CrossEntropyLoss(ignore_index=-100)
                loss = loss_fct(
                    logits.view(-1, self.num_labels),
                    labels.view(-1)
                )
            
            predictions = torch.argmax(logits, dim=-1)
        
        return {
            'loss': loss,
            'logits': logits,
            'predictions': predictions
        }
```

#### 2.2.2 关键技术点

1. **标签mask处理**：
   - padding位置：attention_mask = 0
   - 特殊token和子词：labels = -100
   - 两者结合创建有效mask

2. **CRF集成**：
   - 训练时：计算负对数似然作为损失
   - 推理时：使用Viterbi算法解码最优路径

### 2.3 RoBERTa-NER模型 (roberta_ner.py)

```python
from transformers import RobertaModel

class RobertaNER(BaseNERModel):
    """RoBERTa + CRF NER模型"""
    
    def __init__(self, config: Dict):
        super().__init__(config)
        
        # RoBERTa编码器
        self.roberta = RobertaModel.from_pretrained(
            config['pretrained_model']
        )
        
        # Dropout
        self.dropout = nn.Dropout(config.get('dropout', 0.1))
        
        # 分类层
        self.classifier = nn.Linear(
            self.roberta.config.hidden_size,
            self.num_labels
        )
        
        # CRF层
        self.use_crf = config.get('use_crf', True)
        if self.use_crf:
            self.crf = CRF(self.num_labels, batch_first=True)
    
    def forward(self, input_ids, attention_mask, labels=None):
        """前向传播（与BERT类似）"""
        # 实现逻辑与BertNER基本相同，只是使用RoBERTa编码器
        pass
```

### 2.4 BiLSTM-CRF模型 (bilstm_crf.py)

```python
class BiLSTMCRF(BaseNERModel):
    """BiLSTM + CRF基线模型"""
    
    def __init__(self, config: Dict):
        """
        Args:
            config: {
                'vocab_size': 5000,
                'embedding_dim': 300,
                'hidden_dim': 256,
                'num_layers': 2,
                'dropout': 0.5,
                'num_labels': 9,
                'pretrained_embeddings': None  # 可选的预训练词向量
            }
        """
        super().__init__(config)
        
        # 词嵌入层
        self.embedding = nn.Embedding(
            config['vocab_size'],
            config['embedding_dim'],
            padding_idx=0
        )
        
        # 加载预训练词向量（可选）
        if config.get('pretrained_embeddings') is not None:
            self.embedding.weight.data.copy_(
                torch.from_numpy(config['pretrained_embeddings'])
            )
        
        # BiLSTM层
        self.lstm = nn.LSTM(
            config['embedding_dim'],
            config['hidden_dim'] // 2,  # 双向，所以除以2
            num_layers=config.get('num_layers', 2),
            bidirectional=True,
            batch_first=True,
            dropout=config.get('dropout', 0.5) if config.get('num_layers', 2) > 1 else 0
        )
        
        # Dropout
        self.dropout = nn.Dropout(config.get('dropout', 0.5))
        
        # 分类层
        self.classifier = nn.Linear(config['hidden_dim'], self.num_labels)
        
        # CRF层
        self.crf = CRF(self.num_labels, batch_first=True)
    
    def forward(self, input_ids, attention_mask, labels=None):
        """前向传播"""
        
        # 词嵌入
        embeddings = self.embedding(input_ids)  # [B, L, E]
        embeddings = self.dropout(embeddings)
        
        # BiLSTM
        lstm_out, _ = self.lstm(embeddings)  # [B, L, H]
        lstm_out = self.dropout(lstm_out)
        
        # 分类
        logits = self.classifier(lstm_out)  # [B, L, num_labels]
        
        # CRF
        mask = (attention_mask == 1)
        loss = None
        if labels is not None:
            # 处理标签
            labels_for_crf = labels.clone()
            labels_for_crf[labels == -100] = 0
            mask_for_loss = (labels != -100) & mask
            loss = -self.crf(logits, labels_for_crf, mask=mask_for_loss, reduction='mean')
        
        predictions = torch.tensor(self.crf.decode(logits, mask=mask))
        
        return {
            'loss': loss,
            'logits': logits,
            'predictions': predictions
        }
```

## 3. 训练模块详细设计

### 3.1 训练器 (trainer.py)

```python
from torch.optim import AdamW
from torch.optim.lr_scheduler import get_linear_schedule_with_warmup
from tqdm import tqdm

class NERTrainer:
    """NER训练器"""
    
    def __init__(
        self,
        model: BaseNERModel,
        train_loader: DataLoader,
        val_loader: DataLoader,
        config: Dict,
        device: str = 'cuda'
    ):
        """
        Args:
            model: NER模型
            train_loader: 训练数据加载器
            val_loader: 验证数据加载器
            config: 训练配置
            device: 设备
        """
        self.model = model.to(device)
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.config = config
        self.device = device
        
        # 优化器
        self.optimizer = self._create_optimizer()
        
        # 学习率调度器
        self.scheduler = self._create_scheduler()
        
        # 早停
        self.early_stopping = EarlyStopping(
            patience=config.get('patience', 3),
            mode='max'  # F1-score越大越好
        )
        
        # 训练历史
        self.history = {
            'train_loss': [],
            'val_loss': [],
            'val_f1': [],
            'learning_rate': []
        }
        
        # 最优模型路径
        self.best_model_path = os.path.join(
            config['save_dir'],
            'best_model'
        )
    
    def _create_optimizer(self) -> AdamW:
        """创建优化器"""
        # 分组参数：预训练模型参数 vs 新增参数
        no_decay = ['bias', 'LayerNorm.weight']
        optimizer_grouped_parameters = [
            {
                'params': [p for n, p in self.model.named_parameters()
                          if not any(nd in n for nd in no_decay)],
                'weight_decay': self.config.get('weight_decay', 0.01)
            },
            {
                'params': [p for n, p in self.model.named_parameters()
                          if any(nd in n for nd in no_decay)],
                'weight_decay': 0.0
            }
        ]
        
        optimizer = AdamW(
            optimizer_grouped_parameters,
            lr=self.config.get('learning_rate', 3e-5),
            eps=1e-8
        )
        return optimizer
    
    def _create_scheduler(self):
        """创建学习率调度器"""
        num_training_steps = len(self.train_loader) * self.config['num_epochs']
        num_warmup_steps = int(num_training_steps * self.config.get('warmup_ratio', 0.1))
        
        scheduler = get_linear_schedule_with_warmup(
            self.optimizer,
            num_warmup_steps=num_warmup_steps,
            num_training_steps=num_training_steps
        )
        return scheduler
    
    def train_epoch(self) -> float:
        """训练一个epoch"""
        self.model.train()
        total_loss = 0
        
        progress_bar = tqdm(self.train_loader, desc='Training')
        for batch in progress_bar:
            # 数据移到设备
            input_ids = batch['input_ids'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)
            labels = batch['labels'].to(self.device)
            
            # 前向传播
            outputs = self.model(input_ids, attention_mask, labels)
            loss = outputs['loss']
            
            # 反向传播
            self.optimizer.zero_grad()
            loss.backward()
            
            # 梯度裁剪
            torch.nn.utils.clip_grad_norm_(
                self.model.parameters(),
                self.config.get('max_grad_norm', 1.0)
            )
            
            # 更新参数
            self.optimizer.step()
            self.scheduler.step()
            
            total_loss += loss.item()
            progress_bar.set_postfix({'loss': loss.item()})
        
        avg_loss = total_loss / len(self.train_loader)
        return avg_loss
    
    def evaluate(self, data_loader: DataLoader) -> Tuple[float, float]:
        """评估模型"""
        self.model.eval()
        total_loss = 0
        all_predictions = []
        all_labels = []
        
        with torch.no_grad():
            for batch in tqdm(data_loader, desc='Evaluating'):
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                outputs = self.model(input_ids, attention_mask, labels)
                loss = outputs['loss']
                predictions = outputs['predictions']
                
                total_loss += loss.item()
                
                # 收集预测和标签（排除padding和特殊token）
                for pred, label, mask in zip(predictions, labels, attention_mask):
                    # 只保留有效位置
                    valid_indices = (label != -100) & (mask == 1)
                    all_predictions.append(pred[valid_indices].cpu().numpy())
                    all_labels.append(label[valid_indices].cpu().numpy())
        
        avg_loss = total_loss / len(data_loader)
        
        # 计算F1-score
        from src.evaluation.metrics import compute_metrics
        metrics = compute_metrics(all_predictions, all_labels)
        f1 = metrics['f1']
        
        return avg_loss, f1
    
    def train(self):
        """完整训练流程"""
        print("Starting training...")
        
        for epoch in range(self.config['num_epochs']):
            print(f"\nEpoch {epoch + 1}/{self.config['num_epochs']}")
            
            # 训练
            train_loss = self.train_epoch()
            
            # 验证
            val_loss, val_f1 = self.evaluate(self.val_loader)
            
            # 记录历史
            self.history['train_loss'].append(train_loss)
            self.history['val_loss'].append(val_loss)
            self.history['val_f1'].append(val_f1)
            self.history['learning_rate'].append(
                self.optimizer.param_groups[0]['lr']
            )
            
            print(f"Train Loss: {train_loss:.4f}")
            print(f"Val Loss: {val_loss:.4f}, Val F1: {val_f1:.4f}")
            
            # 保存最优模型
            if val_f1 == max(self.history['val_f1']):
                print(f"New best model! Saving to {self.best_model_path}")
                self.model.save_pretrained(self.best_model_path)
            
            # 早停检查
            self.early_stopping(val_f1)
            if self.early_stopping.should_stop:
                print(f"Early stopping triggered at epoch {epoch + 1}")
                break
        
        print("\nTraining completed!")
        print(f"Best Val F1: {max(self.history['val_f1']):.4f}")
        
        # 保存训练历史
        self._save_history()
    
    def _save_history(self):
        """保存训练历史"""
        history_path = os.path.join(self.config['save_dir'], 'history.json')
        with open(history_path, 'w') as f:
            json.dump(self.history, f, indent=2)
```

### 3.2 早停机制 (utils.py)

```python
class EarlyStopping:
    """早停机制"""
    
    def __init__(self, patience: int = 3, mode: str = 'min', delta: float = 0.0):
        """
        Args:
            patience: 容忍的epoch数
            mode: 'min'表示指标越小越好，'max'表示越大越好
            delta: 最小改进量
        """
        self.patience = patience
        self.mode = mode
        self.delta = delta
        self.counter = 0
        self.best_score = None
        self.should_stop = False
    
    def __call__(self, score: float):
        """更新早停状态"""
        if self.best_score is None:
            self.best_score = score
        elif self._is_improvement(score):
            self.best_score = score
            self.counter = 0
        else:
            self.counter += 1
            if self.counter >= self.patience:
                self.should_stop = True
    
    def _is_improvement(self, score: float) -> bool:
        """判断是否有改进"""
        if self.mode == 'min':
            return score < self.best_score - self.delta
        else:  # mode == 'max'
            return score > self.best_score + self.delta
```

## 4. 评估模块详细设计

### 4.1 评估指标 (metrics.py)

```python
from seqeval.metrics import (
    precision_score,
    recall_score,
    f1_score,
    classification_report
)
from sklearn.metrics import confusion_matrix
import numpy as np

class NERMetrics:
    """NER评估指标"""
    
    def __init__(self, label_encoder: LabelEncoder):
        self.label_encoder = label_encoder
    
    def compute(
        self,
        predictions: List[List[int]],
        labels: List[List[int]]
    ) -> Dict[str, float]:
        """
        计算评估指标
        Args:
            predictions: 预测标签ID列表
            labels: 真实标签ID列表
        Returns:
            评估指标字典
        """
        # 转换为标签字符串
        pred_labels = [
            [self.label_encoder.decode(p) for p in pred]
            for pred in predictions
        ]
        true_labels = [
            [self.label_encoder.decode(l) for l in label]
            for label in labels
        ]
        
        # 计算指标
        precision = precision_score(true_labels, pred_labels)
        recall = recall_score(true_labels, pred_labels)
        f1 = f1_score(true_labels, pred_labels)
        
        # 详细报告
        report = classification_report(
            true_labels,
            pred_labels,
            output_dict=True
        )
        
        return {
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'report': report
        }
    
    def entity_level_accuracy(
        self,
        predictions: List[List[int]],
        labels: List[List[int]]
    ) -> float:
        """计算实体级别准确率"""
        pred_entities = self._extract_entities(predictions)
        true_entities = self._extract_entities(labels)
        
        correct = sum(1 for pe, te in zip(pred_entities, true_entities)
                     if pe == te)
        total = sum(len(te) for te in true_entities)
        
        return correct / total if total > 0 else 0.0
    
    def _extract_entities(
        self,
        label_sequences: List[List[int]]
    ) -> List[Set[Tuple[str, int, int]]]:
        """
        从标签序列中提取实体
        Returns:
            每个句子的实体集合：{(entity_type, start, end), ...}
        """
        all_entities = []
        
        for labels in label_sequences:
            entities = set()
            entity_start = None
            entity_type = None
            
            for i, label_id in enumerate(labels):
                label = self.label_encoder.decode(label_id)
                
                if label.startswith('B-'):
                    # 保存上一个实体
                    if entity_start is not None:
                        entities.add((entity_type, entity_start, i))
                    # 开始新实体
                    entity_type = label[2:]
                    entity_start = i
                elif label.startswith('I-'):
                    # 继续当前实体
                    if entity_start is None:
                        # 错误：I-tag出现但没有B-tag
                        entity_type = label[2:]
                        entity_start = i
                else:  # 'O'
                    # 结束当前实体
                    if entity_start is not None:
                        entities.add((entity_type, entity_start, i))
                        entity_start = None
                        entity_type = None
            
            # 处理句子结尾的实体
            if entity_start is not None:
                entities.add((entity_type, entity_start, len(labels)))
            
            all_entities.append(entities)
        
        return all_entities
```

### 4.2 模型对比器 (evaluator.py)

```python
class ModelComparator:
    """多模型性能对比"""
    
    def __init__(self, label_encoder: LabelEncoder):
        self.label_encoder = label_encoder
        self.metrics_calculator = NERMetrics(label_encoder)
    
    def compare_models(
        self,
        models_results: Dict[str, Tuple[List, List]],
        save_path: str = None
    ) -> pd.DataFrame:
        """
        对比多个模型的性能
        Args:
            models_results: {
                'BERT': (predictions, labels),
                'RoBERTa': (predictions, labels),
                ...
            }
            save_path: 保存结果的路径
        Returns:
            对比结果DataFrame
        """
        results = []
        
        for model_name, (predictions, labels) in models_results.items():
            metrics = self.metrics_calculator.compute(predictions, labels)
            results.append({
                'Model': model_name,
                'Precision': metrics['precision'],
                'Recall': metrics['recall'],
                'F1-Score': metrics['f1']
            })
        
        df = pd.DataFrame(results)
        
        if save_path:
            df.to_csv(save_path, index=False)
        
        return df
```

## 5. 推理模块详细设计

### 5.1 推理器 (predictor.py)

```python
class NERPredictor:
    """NER推理器"""
    
    def __init__(
        self,
        model: BaseNERModel,
        tokenizer,
        label_encoder: LabelEncoder,
        device: str = 'cuda'
    ):
        self.model = model.to(device)
        self.model.eval()
        self.tokenizer = tokenizer
        self.label_encoder = label_encoder
        self.device = device
    
    def predict(self, text: str) -> Dict[str, Any]:
        """
        单句推理
        Args:
            text: 输入文本
        Returns:
            {
                'text': 原文本,
                'tokens': token列表,
                'labels': 标签列表,
                'entities': 实体列表
            }
        """
        # 分词
        encoding = self.tokenizer(
            text,
            return_tensors='pt',
            padding=True,
            truncation=True,
            max_length=128
        )
        
        input_ids = encoding['input_ids'].to(self.device)
        attention_mask = encoding['attention_mask'].to(self.device)
        
        # 推理
        with torch.no_grad():
            predictions = self.model.predict(input_ids, attention_mask)
        
        # 解码
        tokens = self.tokenizer.convert_ids_to_tokens(input_ids[0])
        labels = [self.label_encoder.decode(p.item()) for p in predictions[0]]
        
        # 提取实体
        entities = self._extract_entities(tokens, labels)
        
        return {
            'text': text,
            'tokens': tokens,
            'labels': labels,
            'entities': entities
        }
    
    def predict_batch(
        self,
        texts: List[str],
        batch_size: int = 32
    ) -> List[Dict[str, Any]]:
        """批量推理"""
        results = []
        
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            batch_results = self._predict_batch(batch_texts)
            results.extend(batch_results)
        
        return results
    
    def _predict_batch(self, texts: List[str]) -> List[Dict[str, Any]]:
        """批量推理的内部实现"""
        # 批量分词
        encoding = self.tokenizer(
            texts,
            return_tensors='pt',
            padding=True,
            truncation=True,
            max_length=128
        )
        
        input_ids = encoding['input_ids'].to(self.device)
        attention_mask = encoding['attention_mask'].to(self.device)
        
        # 推理
        with torch.no_grad():
            predictions = self.model.predict(input_ids, attention_mask)
        
        # 解码每个样本
        results = []
        for text, input_id, pred in zip(texts, input_ids, predictions):
            tokens = self.tokenizer.convert_ids_to_tokens(input_id)
            labels = [self.label_encoder.decode(p.item()) for p in pred]
            entities = self._extract_entities(tokens, labels)
            
            results.append({
                'text': text,
                'tokens': tokens,
                'labels': labels,
                'entities': entities
            })
        
        return results
    
    def _extract_entities(
        self,
        tokens: List[str],
        labels: List[str]
    ) -> List[Dict[str, Any]]:
        """
        从token和标签中提取实体
        Returns:
            [{'text': '实体文本', 'type': '实体类型', 'start': 开始位置, 'end': 结束位置}, ...]
        """
        entities = []
        entity_tokens = []
        entity_type = None
        entity_start = None
        
        for i, (token, label) in enumerate(zip(tokens, labels)):
            if label.startswith('B-'):
                # 保存上一个实体
                if entity_tokens:
                    entities.append({
                        'text': self._tokens_to_string(entity_tokens),
                        'type': entity_type,
                        'start': entity_start,
                        'end': i
                    })
                # 开始新实体
                entity_tokens = [token]
                entity_type = label[2:]
                entity_start = i
            elif label.startswith('I-') and entity_tokens:
                # 继续当前实体
                entity_tokens.append(token)
            else:  # 'O' or I- without B-
                # 结束当前实体
                if entity_tokens:
                    entities.append({
                        'text': self._tokens_to_string(entity_tokens),
                        'type': entity_type,
                        'start': entity_start,
                        'end': i
                    })
                    entity_tokens = []
                    entity_type = None
                    entity_start = None
        
        # 处理最后一个实体
        if entity_tokens:
            entities.append({
                'text': self._tokens_to_string(entity_tokens),
                'type': entity_type,
                'start': entity_start,
                'end': len(tokens)
            })
        
        return entities
    
    def _tokens_to_string(self, tokens: List[str]) -> str:
        """将token列表转换为字符串"""
        text = ''.join(tokens)
        # 移除特殊标记
        text = text.replace('##', '')  # BERT的子词标记
        text = text.replace('Ġ', ' ')  # RoBERTa的空格标记
        return text.strip()
```

## 6. 可视化模块详细设计

### 6.1 训练曲线可视化 (plot_metrics.py)

```python
import matplotlib.pyplot as plt
import seaborn as sns

class TrainingVisualizer:
    """训练过程可视化"""
    
    def __init__(self, style: str = 'seaborn'):
        plt.style.use(style)
        sns.set_palette("husl")
    
    def plot_training_history(
        self,
        history: Dict[str, List[float]],
        save_path: str = None
    ):
        """绘制训练历史"""
        fig, axes = plt.subplots(1, 2, figsize=(15, 5))
        
        # 损失曲线
        axes[0].plot(history['train_loss'], label='Train Loss')
        axes[0].plot(history['val_loss'], label='Val Loss')
        axes[0].set_xlabel('Epoch')
        axes[0].set_ylabel('Loss')
        axes[0].set_title('Training and Validation Loss')
        axes[0].legend()
        axes[0].grid(True)
        
        # F1曲线
        axes[1].plot(history['val_f1'], label='Val F1', color='green')
        axes[1].set_xlabel('Epoch')
        axes[1].set_ylabel('F1 Score')
        axes[1].set_title('Validation F1 Score')
        axes[1].legend()
        axes[1].grid(True)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
    
    def plot_model_comparison(
        self,
        comparison_df: pd.DataFrame,
        save_path: str = None
    ):
        """绘制模型对比柱状图"""
        fig, ax = plt.subplots(figsize=(10, 6))
        
        x = np.arange(len(comparison_df))
        width = 0.25
        
        ax.bar(x - width, comparison_df['Precision'], width, label='Precision')
        ax.bar(x, comparison_df['Recall'], width, label='Recall')
        ax.bar(x + width, comparison_df['F1-Score'], width, label='F1-Score')
        
        ax.set_xlabel('Model')
        ax.set_ylabel('Score')
        ax.set_title('Model Performance Comparison')
        ax.set_xticks(x)
        ax.set_xticklabels(comparison_df['Model'])
        ax.legend()
        ax.grid(True, axis='y')
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
```

### 6.2 实体高亮显示 (display_entities.py)

```python
from IPython.display import HTML

class EntityDisplayer:
    """实体可视化展示"""
    
    def __init__(self):
        self.colors = {
            'PER': '#FFB6C1',  # 粉色
            'LOC': '#87CEEB',  # 天蓝色
            'ORG': '#98FB98',  # 淡绿色
            'MISC': '#FFD700'  # 金色
        }
    
    def display_entities(
        self,
        text: str,
        entities: List[Dict[str, Any]],
        show_label: bool = True
    ) -> str:
        """
        生成实体高亮的HTML
        Args:
            text: 原始文本
            entities: 实体列表
            show_label: 是否显示标签
        Returns:
            HTML字符串
        """
        if not entities:
            return text
        
        # 按位置排序
        entities = sorted(entities, key=lambda x: x['start'])
        
        html_parts = []
        last_end = 0
        
        for entity in entities:
            # 添加实体前的文本
            html_parts.append(text[last_end:entity['start']])
            
            # 添加高亮的实体
            color = self.colors.get(entity['type'], '#CCCCCC')
            entity_html = f'<span style="background-color: {color}; padding: 2px 4px; border-radius: 3px;">'
            entity_html += entity['text']
            if show_label:
                entity_html += f'<sub style="font-size: 0.7em; color: #666;"> {entity["type"]}</sub>'
            entity_html += '</span>'
            html_parts.append(entity_html)
            
            last_end = entity['end']
        
        # 添加剩余文本
        html_parts.append(text[last_end:])
        
        return ''.join(html_parts)
    
    def display_jupyter(
        self,
        text: str,
        entities: List[Dict[str, Any]]
    ):
        """在Jupyter中显示"""
        html = self.display_entities(text, entities)
        return HTML(html)
```

## 7. 命令行接口设计

### 7.1 训练脚本 (scripts/train.py)

```python
import argparse

def main():
    parser = argparse.ArgumentParser(description='Train NER model')
    parser.add_argument('--config', type=str, required=True,
                       help='Path to config file')
    parser.add_argument('--model_type', type=str, required=True,
                       choices=['bert', 'roberta', 'bilstm'],
                       help='Model type')
    parser.add_argument('--language', type=str, required=True,
                       choices=['chinese', 'english'],
                       help='Language')
    parser.add_argument('--device', type=str, default='cuda',
                       help='Device to use')
    
    args = parser.parse_args()
    
    # 加载配置
    config = load_config(args.config)
    
    # 准备数据
    # ...
    
    # 创建模型
    # ...
    
    # 训练
    # ...

if __name__ == '__main__':
    main()
```

### 7.2 推理脚本 (scripts/predict.py)

```python
def main():
    parser = argparse.ArgumentParser(description='NER Prediction')
    parser.add_argument('--model_path', type=str, required=True,
                       help='Path to trained model')
    parser.add_argument('--text', type=str,
                       help='Text to predict')
    parser.add_argument('--input_file', type=str,
                       help='Input file with texts')
    parser.add_argument('--output_file', type=str,
                       help='Output file for results')
    parser.add_argument('--device', type=str, default='cuda')
    
    args = parser.parse_args()
    
    # 加载模型和预测
    # ...

if __name__ == '__main__':
    main()
```

## 8. 总结

本详细设计文档涵盖了系统的所有核心模块：

1. **数据预处理模块**：数据加载、Dataset实现、标签编码
2. **模型模块**：BERT、RoBERTa、BiLSTM-CRF的详细实现
3. **训练模块**：完整的训练器、优化器配置、早停机制
4. **评估模块**：指标计算、模型对比
5. **推理模块**：单句和批量推理实现
6. **可视化模块**：训练曲线、模型对比、实体展示

每个模块都提供了：
- 清晰的类设计和接口定义
- 详细的参数说明和返回值
- 关键算法的实现逻辑
- 实用的工具函数

为代码实现提供了完整的技术规格。
