# RoBERTa中文NER配置文件
model:
    name: 'roberta'
    type: 'roberta'
    pretrained_model: 'hfl/chinese-roberta-wwm-ext'
    num_labels: 9
    dropout: 0.1
    use_crf: true

training:
    batch_size: 16
    learning_rate: 2e-5
    num_epochs: 10
    warmup_ratio: 0.1
    weight_decay: 0.01
    max_grad_norm: 1.0
    patience: 3

data:
    language: 'chinese'
    train_file: 'data/raw/chinese/train.txt'
    dev_file: 'data/raw/chinese/dev.txt'
    test_file: 'data/raw/chinese/test.txt'
    max_length: 128

output:
    save_dir: 'experiments/roberta_chinese'
    log_file: 'experiments/roberta_chinese/training.log'

device: 'cuda'
seed: 42
